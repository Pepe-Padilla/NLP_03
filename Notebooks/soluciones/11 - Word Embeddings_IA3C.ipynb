{"cells":[{"cell_type":"markdown","metadata":{"id":"TUh0d6qMgMB0"},"source":["## Word2Vec\n","\n","El modelo más famoso de word embeddings. Inventado por [Tomas Mikolov en 2013 en Google](https://arxiv.org/pdf/1310.4546.pdf)\n","\n","https://radimrehurek.com/gensim/models/word2vec.html\n","\n","### Hyperparameters\n","\n","Algunos de los hiperparámetros que deberemos tener en cuenta y configuraremos:\n","\n","- size: dimensionalidad de las palabras vector\n","- window: ventana para obtener el contexto de cada palabra. Se mide en número de palabras máximo entre la palabra actual y la palabra a predecir\n","- min_count: frecuencia mínima de aparición de una palabra para que sea considerada en el entrenamiento\n","- sg: algoritmo escogido. 1 para Skip-Gram, 0 para CBOW\n","- hs: si = 1, se utiliza una softmax jerárquica. Si = 0, y negative != 0, se emplea negative sampling\n","- negative: si = 0, no se usa negative sampling. Si es > 0, se usará negative sampling. El valor indica el número de \"palabras ruidosas\" se incluirán (usual entre 5-20 para datasets pequeños, entre 2-5 para datasets grandes)\n","\n","### Noción de contexto\n","\n","<img src=https://docs.chainer.org/en/v4.0.0b2/_images/center_context_word.png width=450px>\n","\n","## Atributos\n","\n","- wv: word vectors, contiene el mapeo entre palabras y vectores (embeddings)\n","- vocabulary: vocabulario (o diccionario) del modelo\n","\n","\n","## Negative sampling\n","\n","Cuando se trabaja en NLP el tamaño del vocabulario suele tener una cardinalidad enorme. Esto afecta a los modelos de lenguaje a la hora de predecir aquellas palabras que, aunque correctas, no son demasiado frecuentes.\n","\n","Además, contextos muy comunes (como los que podrían ser aquellos en los que se encuentran muchas stop words) hacen que el entrenamiento sea lento. Se emplea, por tanto, para reducir la carga computacional al problema.\n","\n","La solución que se propone - e implementa - Word2Vec es que cada palabra tenga una determinada probabilidad de ser eliminada del training set. Dicha probabilidad estará relacionada con la frecuencia de repetición de dicha palabra.\n","\n","\n","## Arquitecturas\n","\n","Existen dos arquitecturas de este modelo: CBOW y Skip Gram.\n","\n","\n","#### CBOW (Continuous Bag of Words)\n","\n","Durante el entrenamiento, el modelo tratará de **predecir la palabra actual** dado el contexto en el que se encuentre. La capa de entrada contendrá las palabras-contexto y la de salida será la palabra actual (o palabra a predecir). La capa intermedia tendrá una dimension igual al número de dimensiones en el que queremos representar la palabra actual a la salida.\n","\n","<img src=https://miro.medium.com/max/1104/0*CCsrTAjN80MqswXG width=400px>\n","\n","\n","#### Skip Gram\n","\n","Durante el entrenamiento, el modelo tratará de predecir **el contexto (palabras-contexto)** a una palabra dada. La capa de entrada contendrá la palabra actual y la de salida serán las palabras contexto. La capa intermedia es análoga a la presente en la arquitectura CBOW.\n","\n","<img src=https://miro.medium.com/max/1280/0*Ta3qx5CQsrJloyCA.png width=300px>\n","\n","### ¿Cuál es mejor?\n","\n","En general, depende. CBOW, al haber sido entrenado para predecir una palabra dado un contexto, será algo mejor _rellenando huecos_, aunque eso puede significar que palabras correctas pero menos comunes no aparezcan como resultado algunas veces. Skip Gram, en cambio, debería ser mejor infiriendo relaciones más concretas en contextos similares (por ejemplo, \"me gusta el color verde\" y \"me encanta el color azul\", y la diferencia en la intensidad del sentimiento expresado).\n","\n","Si atendemos a los comentarios de Mikolov:\n","\n","- Skip-gram: funciona bien con conjuntos de datos pequeños, representando bien incluso palabras o frases extrañas (poco comunes)\n","- CBOW: entrenamiento varias veces más rápido, su performance mejor para aquellas palabras más frecuentes que el resto"]},{"cell_type":"markdown","metadata":{"id":"t2gRfi5PgMBx"},"source":["# Word Embeddings\n","\n","Los Word Embeddings son aquellas técnicas y modelos de lenguaje que permiten mapear palabras a vectores de valores continuos. Durante el entrenamiento de dichos vectores se buscará que capturen la información semántica de las palabras.\n","\n","<img src=https://miro.medium.com/max/1280/1*OEmWDt4eztOcm5pr2QbxfA.png widt=500px>\n","\n","Gracias a que los vectores tienen información sobre la semántica, mediante operaciones vectoriales podemos encontrar palabras (o documentos) que tienen un significado similar.\n","\n","La idea principal de este tipo de modelos es que **palabras que aparecen en contextos similares tienen semánticas similares**. **Concepto de sustituibilidad**.\n","\n","**Aquellas palabras que semánticamente son similares tendrán - idealmente - vectores-palabra cercanos entre sí.**\n","\n","Existen multitud de modelos, en esta sesión veremos solo algunas."]},{"cell_type":"code","source":["!pip install unzip\n","!unzip simpsons_dataset.csv.zip"],"metadata":{"id":"B94wklRcPd9D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718913946692,"user_tz":-120,"elapsed":11146,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"31c98d4e-5e92-4ee8-d844-86c9d17d876e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unzip\n","  Downloading unzip-1.0.0.tar.gz (704 bytes)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: unzip\n","  Building wheel for unzip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unzip: filename=unzip-1.0.0-py3-none-any.whl size=1283 sha256=1491f4fb4aa57594c2e979d3b59ddcfce6f2e88e3822e99b1561cef3d5b06224\n","  Stored in directory: /root/.cache/pip/wheels/80/dc/7a/f8af45bc239e7933509183f038ea8d46f3610aab82b35369f4\n","Successfully built unzip\n","Installing collected packages: unzip\n","Successfully installed unzip-1.0.0\n","unzip:  cannot find or open simpsons_dataset.csv.zip, simpsons_dataset.csv.zip.zip or simpsons_dataset.csv.zip.ZIP.\n"]}]},{"cell_type":"markdown","metadata":{"id":"_lSrgrn6gMB1"},"source":["## Palabras más similares"]},{"cell_type":"markdown","metadata":{"id":"zi48GKy7gMB2"},"source":["## Importamos las librerías"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Z6fPTAKYgMB2","executionInfo":{"status":"ok","timestamp":1718914000411,"user_tz":-120,"elapsed":1991,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"outputs":[],"source":["#Ejercicio de aplicación con Word2Vec\n","from gensim.models import Word2Vec\n","from gensim.models.word2vec import LineSentence\n","\n"]},{"cell_type":"code","source":["import re  # Para Preprocesamiento\n","import pandas as pd\n","from time import time  # Tiempo de las operaciones\n","from collections import defaultdict  # Para Frecuencias de palabras\n","\n","import spacy  # Para prepocesamiento\n","import logging  # Configuración de loggings para monitor gensim\n","logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n","from gensim.models.phrases import Phrases, Phraser"],"metadata":{"id":"n6KlKP0Ixqrf","executionInfo":{"status":"ok","timestamp":1718914044744,"user_tz":-120,"elapsed":7581,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Lectura de datos"],"metadata":{"id":"jb0ye5BkE6ZC"}},{"cell_type":"code","source":["df = pd.read_csv('./simpsons_dataset.csv')\n","df.shape"],"metadata":{"id":"49WrIu_xxquV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914087642,"user_tz":-120,"elapsed":250,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"d653cb59-0133-46f2-a720-b2cad4256f6a"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(106730, 2)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"dtOqm3__PhXd","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1718914099007,"user_tz":-120,"elapsed":253,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"87b93a67-df69-4832-c462-f130e68713d9"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        raw_character_text                                       spoken_words\n","0              Miss Hoover  No, actually, it was a little of both. Sometim...\n","1             Lisa Simpson                             Where's Mr. Bergstrom?\n","2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n","3             Lisa Simpson                         That life is worth living.\n","4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."],"text/html":["\n","  <div id=\"df-31074703-8a74-484e-aa10-40bae5164d7f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>raw_character_text</th>\n","      <th>spoken_words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Miss Hoover</td>\n","      <td>No, actually, it was a little of both. Sometim...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lisa Simpson</td>\n","      <td>Where's Mr. Bergstrom?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Miss Hoover</td>\n","      <td>I don't know. Although I'd sure like to talk t...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Lisa Simpson</td>\n","      <td>That life is worth living.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Edna Krabappel-Flanders</td>\n","      <td>The polls will be open from now until the end ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31074703-8a74-484e-aa10-40bae5164d7f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-31074703-8a74-484e-aa10-40bae5164d7f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-31074703-8a74-484e-aa10-40bae5164d7f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-75e719d1-51b1-43c0-bd04-f80d6f3f687d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-75e719d1-51b1-43c0-bd04-f80d6f3f687d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-75e719d1-51b1-43c0-bd04-f80d6f3f687d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"BxmY6TDHPj-p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914113416,"user_tz":-120,"elapsed":277,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"3e7d7600-11bb-465e-aedf-ff28569d7d07"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["raw_character_text    12102\n","spoken_words          17731\n","dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["df = df.dropna().reset_index(drop=True)\n","df.isnull().sum()"],"metadata":{"id":"WSsEl1tzPmbC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914116083,"user_tz":-120,"elapsed":228,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"f463b016-2989-4232-b234-0bc088d03d7f"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["raw_character_text    0\n","spoken_words          0\n","dtype: int64"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["\n","nlp = spacy.load(\"en_core_web_sm\")\n","def cleaning(doc):\n","    # Lematizamos y removemos stopwords\n","    # doc necesita ser a spacy Doc object\n","    txt = [token.lemma_ for token in doc if not token.is_stop]\n","    # Word2Vec usa las palabras de contexto para aprender a representar el vector de una palabra ,\n","    # si una sentencia tiene solo una o dos palabras ,\n","    # el beneficio para el training es muy pequeño\n","    if len(txt) > 2:\n","        return ' '.join(txt)"],"metadata":{"id":"eFTJ80iGxq5B","executionInfo":{"status":"ok","timestamp":1718914168790,"user_tz":-120,"elapsed":1251,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Quitar los caracteres no alfabéticos"],"metadata":{"id":"OQhmy7f50TUP"}},{"cell_type":"code","source":["brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['spoken_words'])"],"metadata":{"id":"Gf5dPnngxq7n","executionInfo":{"status":"ok","timestamp":1718914177403,"user_tz":-120,"elapsed":235,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Utilizamos el atributo .pipe() de spaCy para acelerar la velocidad del proceso de limpieza"],"metadata":{"id":"z9EZxwLw0iaQ"}},{"cell_type":"code","source":["t = time()\n","\n","txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n","\n","print('Tiempo para limpiar todo: {} mins'.format(round((time() - t) / 60, 2)))"],"metadata":{"id":"Em0MVV0Lxq-O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914372223,"user_tz":-120,"elapsed":150128,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"9e36d54a-0924-4468-f5eb-1bd4c3be67de"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Tiempo para limpiar todo: 2.5 mins\n"]}]},{"cell_type":"markdown","source":["Organizamos el resultado en una DataFrame eliminando los valores missing y duplicados:"],"metadata":{"id":"zN2mYGM300vb"}},{"cell_type":"code","source":["df_clean_simpsons = pd.DataFrame({'clean': txt})\n","df_clean_simpsons = df_clean_simpsons.dropna().drop_duplicates()\n","df_clean_simpsons.shape"],"metadata":{"id":"zmMEpog50lQi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914446340,"user_tz":-120,"elapsed":252,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"e71ac5ea-c0cc-4939-f325-7af660a7898d"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(58381, 1)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["df_clean_simpsons.to_csv('./df_clean_simpsons.csv')"],"metadata":{"id":"rHgB5MNVMc6j","executionInfo":{"status":"ok","timestamp":1718914468262,"user_tz":-120,"elapsed":259,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## Entrenamos el modelo"],"metadata":{"id":"EaQmDpUV23gj"}},{"cell_type":"code","source":["import multiprocessing\n","\n","from gensim.models import Word2Vec"],"metadata":{"id":"SsDfbxj7z3nl","executionInfo":{"status":"ok","timestamp":1718914508807,"user_tz":-120,"elapsed":244,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["Separamos el training del modelo en tres pasos:\n","\n","Word2Vec():\n","- En el primer paso preparamos los parametros del modelo  \n","\n",".build_vocab():\n","- En este paso se construye el vocabulario de una secuencia de sentencias y así inicializamos el modelo. Con los loggings, podemos ver el efecto el efecto de   min_count y sample  sobre el word corpus. Estos parametros, en particular sample, tienen una gran influencia sobre el performance del modelo.\n","\n",".train():\n","- Finalmente, entrenamos el modelo.\n","El loggings aquí puede ser útil para ir monitoriando."],"metadata":{"id":"WDEkp3k63iVS"}},{"cell_type":"code","source":["cores = multiprocessing.cpu_count() # Contamos el número de cores en el ordenador\n","print (cores)"],"metadata":{"id":"By6JsKbz0D26","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914573158,"user_tz":-120,"elapsed":245,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"a40a02e2-0100-4e07-d443-31ce70175061"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n"]}]},{"cell_type":"markdown","source":["## Hyperparameters e Inicializamos los objetos Word2Vec"],"metadata":{"id":"EZ6GQAMsFEVk"}},{"cell_type":"code","source":["w2v_model = Word2Vec(min_count=20,\n","                     window=2,\n","                     vector_size=300,\n","                     sample=6e-5,\n","                     alpha=0.03,\n","                     min_alpha=0.0007,\n","                     negative=20,\n","                     workers=cores-1)"],"metadata":{"id":"_jAey-hjz3qS","executionInfo":{"status":"ok","timestamp":1718914671884,"user_tz":-120,"elapsed":227,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## Construímos el vocabulario"],"metadata":{"id":"VAskuSmlFQmJ"}},{"cell_type":"markdown","source":["Word2Vec requiere construir una tabla del vocabulario (procesamos todas las palabras, filtramos y las contamos):"],"metadata":{"id":"oFjsiXM-5vZs"}},{"cell_type":"code","source":["t = time()\n","sent = [row.split() for row in df_clean_simpsons['clean']]\n","w2v_model.build_vocab(sent, progress_per=10000)\n","\n","print('Tiempo para construir el vocabulario: {} mins'.format(round((time() - t) / 60, 2)))\n"],"metadata":{"id":"_ukURhOcz3tB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914716225,"user_tz":-120,"elapsed":636,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"a1b131ef-b3b5-43a0-d367-aec38d8492b3"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Tiempo para construir el vocabulario: 0.01 mins\n"]}]},{"cell_type":"code","source":["print('Vocabulario compuesto por {} palabras'.format(len(w2v_model.wv.key_to_index)))\n","\n"],"metadata":{"id":"QGjRiPZtBnYx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914721353,"user_tz":-120,"elapsed":268,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"8cfc148e-2b60-4aae-ea8f-d1cd8689b92a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulario compuesto por 2399 palabras\n"]}]},{"cell_type":"markdown","source":["## Entrenamos el modelo"],"metadata":{"id":"dkFW8H_vGddp"}},{"cell_type":"markdown","source":["Parametros del training:\n","\n","* total_examples = int - cuenta las sentencias\n","* epochs = int - Número de iteraciones (epochs) sobre el corpus - [10, 20, 30]"],"metadata":{"id":"g6Wc_KHw7PL9"}},{"cell_type":"code","source":["t = time()\n","\n","w2v_model.train(sent, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n","\n","\n","print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"],"metadata":{"id":"PVKlCaFjxrAs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914776737,"user_tz":-120,"elapsed":30386,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"1d3753c0-bb2c-4033-a2e9-091a07331493"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Time to train the model: 0.5 mins\n"]}]},{"cell_type":"markdown","source":["## Guardamos los modelos"],"metadata":{"id":"aPfyehz-GjHT"}},{"cell_type":"code","source":["w2v_model.save('./w2v_model.pkl')\n","\n"],"metadata":{"id":"1KZaT3cZCOHn","executionInfo":{"status":"ok","timestamp":1718914862715,"user_tz":-120,"elapsed":245,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["## Algunos resultados"],"metadata":{"id":"hmJnOk3tHZxw"}},{"cell_type":"markdown","source":["* Similares:\n","\n","Preguntamos a nuestros modelo encontrar la palabra más similar a los personajes más populares de the Simpsons!"],"metadata":{"id":"i5REdF-C709v"}},{"cell_type":"code","source":["w2v_model.wv.most_similar(positive=[\"homer\"])\n"],"metadata":{"id":"3IcEzYCIPved","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914919692,"user_tz":-120,"elapsed":333,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"267bcfc5-ee4a-42cc-a72c-af8ec6fa33c3"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('marge', 0.8574258089065552),\n"," ('rude', 0.812406599521637),\n"," ('sweetheart', 0.8062543272972107),\n"," ('sorry', 0.8028982281684875),\n"," ('mention', 0.8028407096862793),\n"," ('becky', 0.8023096919059753),\n"," ('bartender', 0.7974420785903931),\n"," ('care', 0.7960628867149353),\n"," ('tell', 0.7945667505264282),\n"," ('happen', 0.7928844690322876)]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["w2v_model.wv.most_similar(positive=[\"marge\"])"],"metadata":{"id":"w242_nZlPyoF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914968458,"user_tz":-120,"elapsed":248,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"cfa2b666-e519-4bd9-ffd5-864a8cf510a8"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('glad', 0.8728121519088745),\n"," ('homie', 0.8682390451431274),\n"," ('becky', 0.8655655384063721),\n"," ('ashamed', 0.8582256436347961),\n"," ('homer', 0.8574257493019104),\n"," ('jealous', 0.8534492254257202),\n"," ('talk', 0.8505071401596069),\n"," ('son', 0.8467366695404053),\n"," ('wonderful', 0.8453777432441711),\n"," ('happen', 0.8449551463127136)]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["w2v_model.wv.most_similar(positive=[\"bart\"])"],"metadata":{"id":"GUip5D4jP1-X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914982379,"user_tz":-120,"elapsed":286,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"f327215f-7dcb-441c-c1b5-dbeaa02bb761"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('lisa', 0.8813179731369019),\n"," ('worried', 0.8565447926521301),\n"," ('jealous', 0.8541589379310608),\n"," ('upset', 0.8430203795433044),\n"," ('surprised', 0.8416628837585449),\n"," ('concerned', 0.8363306522369385),\n"," ('glad', 0.8341752290725708),\n"," ('nervous', 0.8334977030754089),\n"," ('affair', 0.8275941014289856),\n"," ('humiliate', 0.8273479342460632)]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["w2v_model.wv.similarity('maggie', 'baby')"],"metadata":{"id":"W8deChhdP5H5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718914999779,"user_tz":-120,"elapsed":352,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"2c2a9c9d-38d1-44f3-b381-1ba83ace3844"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7933134"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["w2v_model.wv.similarity('bart', 'nelson')"],"metadata":{"id":"KkwRQ4s5P7Mt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718915001525,"user_tz":-120,"elapsed":243,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"6baef9b2-7850-42e3-89d3-189830947415"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.73171425"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["w2v_model.wv.doesnt_match(['jimbo', 'milhouse'])"],"metadata":{"id":"k-B5ZqTPP9tS","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1718915003419,"user_tz":-120,"elapsed":242,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"38609ca5-dd97-4110-f4ca-c5759bd5afee"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'jimbo'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["w2v_model.wv.doesnt_match(['homer', 'patty', 'selma'])"],"metadata":{"id":"tG1WWiCeQBnC","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1718915008683,"user_tz":-120,"elapsed":247,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"5958e028-10e9-4404-bf08-12d37ebeeaf2"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'homer'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["## Palabras fuera del vocabulario (OOV Words)\n","\n","Los embeddings calculados a nivel de palabra no devolverán un vector para aquellos tokens que no hayan guardado en su vocabulario. Una vez que los vectores palabra han sido aprendidos, aquellas palabras que no han sido aprendidas durante el entrenamiento no tendrán representación."],"metadata":{"id":"Aznf4EEjHib_"}},{"cell_type":"code","source":["'asereje' in w2v_model.wv.key_to_index"],"metadata":{"id":"i0NdHBQRQE8J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718915098568,"user_tz":-120,"elapsed":245,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"120d0cbc-94e9-44e5-d56d-ef833440dced"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["w2v_model.wv.most_similar('asereje')"],"metadata":{"id":"S7tOzvMGQHQM","colab":{"base_uri":"https://localhost:8080/","height":304},"executionInfo":{"status":"error","timestamp":1718915119802,"user_tz":-120,"elapsed":446,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"4dbd514e-2e16-4e2e-e728-495ab4e45667"},"execution_count":30,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"\"Key 'asereje' not present in vocabulary\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-42640da976bf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'asereje'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"Key 'asereje' not present in vocabulary\""]}]},{"cell_type":"code","source":[],"metadata":{"id":"jE1gi1HuXiYa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CyMvQlCLgMB6"},"source":["## Visualización: Bonus\n","\n","[Enlace](https://anvaka.github.io/pm/#/galaxy/word2vec-wiki?cx=-16179&cy=-1641&cz=4313&lx=0.3194&ly=-0.5230&lz=-0.4110&lw=0.6749&ml=300&s=1.75&l=1&v=d50_clean)\n","\n","<img src=https://empresas.blogthinkbig.com/wp-content/uploads/2019/06/embeddings_galaxy.png width=650px>"]},{"cell_type":"markdown","source":["Algunas estrategias para lidiar con palabras OOV:\n","- Asignar un vector que siga una distribución aleatoria uniforme. P. ej.:\n","`unk = np.random.uniform(-np.var(w2v.wv.vectors), np.var(w2v.wv.vectors), w2v.wv.vector_size)`\n","- Reemplazar por un token especial, conocido y distinto del resto, (`<unk>`) y entrenar los embeddings\n","- Reemplazar por un token especial, conocido y disinto del resto, y añadir información extra. P. ej.: `<unk_noun>` o `<unk_verb>`\n","- Utilizar modelos que no sean a nivel de palabra"],"metadata":{"id":"jgK4Dpz9SIZc"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}