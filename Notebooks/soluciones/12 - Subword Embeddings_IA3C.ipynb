{"cells":[{"cell_type":"markdown","metadata":{"id":"eJBuKgYggV6W"},"source":["# FastText\n","\n","A diferencia de Word2Vec, que trabaja a nivel de palabra, FastText trata de capturar la información morfológica de las palabras.\n","\n",">*\"[...] we propose a new approach **based on the skipgram model, where each word is represented as a bag of character n-grams**. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. [...]\"* <br>(Mikolov et al., Enriching Word Vectors with Subword Information, https://arxiv.org/pdf/1607.04606.pdf)\n","\n","De esta manera, una palabra quedará representada por sus n-grams.\n","\n","El tamaño de los n-grams deberá ser definido como hiperparámetro\n","- min_n: valor mínimo de _n_ a considerar\n","- max_n: valor máximo de _n_ a considerar\n","\n","Ejemplo:\n",">*\"Me gusta el procesado del lenguaje natural\"*\n",">* Ejemplo de *skip-gram* pre-procesado con una ventana de contexto de 2 palabras\n",">\n",">$w_{target} =$ \"procesado\" &emsp;$w_{context} =$ [\"gusta\", \"el\", \"del\", \"lenguaje\"]\n",">\n",">     (\"procesado\", \"gusta\")\n",">\n","> Descomoposición de n-grams con min_n=3 and max_n=4:\n",">\n",">\"procesado\" = [\"$<$pr\", \"pro\", ..., \"ado\", \"do$>$\", \"$<$pro\", \"roce\", ..., \"sado\", \"ado$>$\"]\n",">\n",">* De este modo, la similitud será: <br><br>\n",">&emsp;$\\boxed{s(w_{target}, w_{context}) = \\sum_{g \\in G_{w_{target}}}z_{g}^T v_{w_{context}}}$, where $G_{w_{target}}\\subset\\{g_{1}, ..., g_{G}\\}$"]},{"cell_type":"markdown","metadata":{"id":"V0qLjFS_gV6a"},"source":["## Palabras más similares"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"VTzIvoQ-gV6b","executionInfo":{"status":"ok","timestamp":1718916822305,"user_tz":-120,"elapsed":3,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"outputs":[],"source":["def print_sim_words(word, model1, model2):\n","    query = \"Most similar to {}\".format(word)\n","    print(query)\n","    print(\"-\"*len(query))\n","    for (sim1, sim2) in zip(model1.wv.most_similar(word), model2.wv.most_similar(word)):\n","        print(\"{}:{}{:.3f}{}{}:{}{:.3f}\".format(sim1[0],\n","                                               \" \"*(20-len(sim1[0])),\n","                                               sim1[1],\n","                                               \" \"*10,\n","                                               sim2[0],\n","                                               \" \"*(20-len(sim2[0])),\n","                                               sim2[1]))\n","    print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"4GYhLbgugV6c"},"source":["## Importamos las librerías"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"LD5YKchbgV6c","executionInfo":{"status":"ok","timestamp":1718916830316,"user_tz":-120,"elapsed":5271,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"outputs":[],"source":["\n","from gensim.models import FastText\n","from gensim.models.word2vec import LineSentence\n","from gensim.models.phrases import Phrases, Phraser"]},{"cell_type":"markdown","metadata":{"id":"RX8m7DX1gV6c"},"source":["## Lectura de datos"]},{"cell_type":"code","source":["!pip install unzip\n","!unzip df_clean_simpsons.csv.zip"],"metadata":{"id":"j_XwCiBWEqXU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718916885464,"user_tz":-120,"elapsed":9938,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"0e2bcc19-db42-4364-e1d9-029b833b8878"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unzip\n","  Downloading unzip-1.0.0.tar.gz (704 bytes)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: unzip\n","  Building wheel for unzip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unzip: filename=unzip-1.0.0-py3-none-any.whl size=1283 sha256=0278517a38ea53bc1f8c64ccf2e1755546705db1d5f2c7648b18c12d3ec30712\n","  Stored in directory: /root/.cache/pip/wheels/80/dc/7a/f8af45bc239e7933509183f038ea8d46f3610aab82b35369f4\n","Successfully built unzip\n","Installing collected packages: unzip\n","Successfully installed unzip-1.0.0\n","unzip:  cannot find or open df_clean_simpsons.csv.zip, df_clean_simpsons.csv.zip.zip or df_clean_simpsons.csv.zip.ZIP.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","df_clean = pd.read_csv('./df_clean_simpsons.csv')"],"metadata":{"id":"QXn1Q5bgOKPT","executionInfo":{"status":"ok","timestamp":1718916945575,"user_tz":-120,"elapsed":343,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","sent = [row.split() for row in df_clean['clean']]"],"metadata":{"id":"d1GDH8oeOd5G","executionInfo":{"status":"ok","timestamp":1718916951066,"user_tz":-120,"elapsed":355,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DzpsA8BpgV6d"},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"WsUJBv_igV6d","executionInfo":{"status":"ok","timestamp":1718916981280,"user_tz":-120,"elapsed":361,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"outputs":[],"source":["sg_params = {\n","    'sg': 1,\n","    'vector_size': 300,\n","    'min_count': 5,\n","    'window': 5,\n","    'hs': 0,\n","    'negative': 20,\n","    'workers': 4,\n","    'min_n': 3,\n","    'max_n': 6\n","}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UT5AbNulgV6d"},"source":["## Inicializamos el objeto FastText"]},{"cell_type":"code","source":["help(FastText)"],"metadata":{"id":"Y_uqb5bjggc4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718916994681,"user_tz":-120,"elapsed":430,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"32f08098-1de7-40d4-8b78-29476ba504f0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class FastText in module gensim.models.fasttext:\n","\n","class FastText(gensim.models.word2vec.Word2Vec)\n"," |  FastText(sentences=None, corpus_file=None, sg=0, hs=0, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), max_final_vocab=None, shrink_windows=True)\n"," |  \n"," |  Method resolution order:\n"," |      FastText\n"," |      gensim.models.word2vec.Word2Vec\n"," |      gensim.utils.SaveLoad\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, sentences=None, corpus_file=None, sg=0, hs=0, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), max_final_vocab=None, shrink_windows=True)\n"," |      Train, use and evaluate word representations learned using the method\n"," |      described in `Enriching Word Vectors with Subword Information <https://arxiv.org/abs/1607.04606>`_,\n"," |      aka FastText.\n"," |      \n"," |      The model can be stored/loaded via its :meth:`~gensim.models.fasttext.FastText.save` and\n"," |      :meth:`~gensim.models.fasttext.FastText.load` methods, or loaded from a format compatible with the\n"," |      original Fasttext implementation via :func:`~gensim.models.fasttext.load_facebook_model`.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      sentences : iterable of list of str, optional\n"," |          Can be simply a list of lists of tokens, but for larger corpora,\n"," |          consider an iterable that streams the sentences directly from disk/network.\n"," |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus'\n"," |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such\n"," |          examples. If you don't supply `sentences`, the model is left uninitialized -- use if you plan to\n"," |          initialize it in some other way.\n"," |      corpus_file : str, optional\n"," |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n"," |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n"," |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left\n"," |          uninitialized).\n"," |      min_count : int, optional\n"," |          The model ignores all words with total frequency lower than this.\n"," |      vector_size : int, optional\n"," |          Dimensionality of the word vectors.\n"," |      window : int, optional\n"," |          The maximum distance between the current and predicted word within a sentence.\n"," |      workers : int, optional\n"," |          Use these many worker threads to train the model (=faster training with multicore machines).\n"," |      alpha : float, optional\n"," |          The initial learning rate.\n"," |      min_alpha : float, optional\n"," |          Learning rate will linearly drop to `min_alpha` as training progresses.\n"," |      sg : {1, 0}, optional\n"," |          Training algorithm: skip-gram if `sg=1`, otherwise CBOW.\n"," |      hs : {1,0}, optional\n"," |          If 1, hierarchical softmax will be used for model training.\n"," |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n"," |      seed : int, optional\n"," |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n"," |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n"," |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n"," |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n"," |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n"," |      max_vocab_size : int, optional\n"," |          Limits the RAM during vocabulary building; if there are more unique\n"," |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n"," |          Set to `None` for no limit.\n"," |      sample : float, optional\n"," |          The threshold for configuring which higher-frequency words are randomly downsampled,\n"," |          useful range is (0, 1e-5).\n"," |      negative : int, optional\n"," |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n"," |          should be drawn (usually between 5-20).\n"," |          If set to 0, no negative sampling is used.\n"," |      ns_exponent : float, optional\n"," |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n"," |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n"," |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n"," |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n"," |          other values may perform better for recommendation applications.\n"," |      cbow_mean : {1,0}, optional\n"," |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n"," |      hashfxn : function, optional\n"," |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n"," |      iter : int, optional\n"," |          Number of iterations (epochs) over the corpus.\n"," |      trim_rule : function, optional\n"," |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n"," |          be trimmed away, or handled using the default (discard if word count < min_count).\n"," |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n"," |          or a callable that accepts parameters (word, count, min_count) and returns either\n"," |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n"," |          The rule, if given, is only used to prune vocabulary during\n"," |          :meth:`~gensim.models.fasttext.FastText.build_vocab` and is not stored as part of themodel.\n"," |      \n"," |          The input parameters are of the following types:\n"," |              * `word` (str) - the word we are examining\n"," |              * `count` (int) - the word's frequency count in the corpus\n"," |              * `min_count` (int) - the minimum count threshold.\n"," |      \n"," |      sorted_vocab : {1,0}, optional\n"," |          If 1, sort the vocabulary by descending frequency before assigning word indices.\n"," |      batch_words : int, optional\n"," |          Target size (in words) for batches of examples passed to worker threads (and\n"," |          thus cython routines).(Larger batches will be passed if individual\n"," |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n"," |      min_n : int, optional\n"," |          Minimum length of char n-grams to be used for training word representations.\n"," |      max_n : int, optional\n"," |          Max length of char ngrams to be used for training word representations. Set `max_n` to be\n"," |          lesser than `min_n` to avoid char ngrams being used.\n"," |      word_ngrams : int, optional\n"," |          In Facebook's FastText, \"max length of word ngram\" - but gensim only supports the\n"," |          default of 1 (regular unigram word handling).\n"," |      bucket : int, optional\n"," |          Character ngrams are hashed into a fixed number of buckets, in order to limit the\n"," |          memory usage of the model. This option specifies the number of buckets used by the model.\n"," |          The default value of 2000000 consumes as much memory as having 2000000 more in-vocabulary\n"," |          words in your model.\n"," |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n"," |          List of callbacks that need to be executed/run at specific stages during training.\n"," |      max_final_vocab : int, optional\n"," |          Limits the vocab to a target vocab size by automatically selecting\n"," |          ``min_count```.  If the specified ``min_count`` is more than the\n"," |          automatically calculated ``min_count``, the former will be used.\n"," |          Set to ``None`` if not required.\n"," |      shrink_windows : bool, optional\n"," |          New in 4.1. Experimental.\n"," |          If True, the effective window size is uniformly sampled from  [1, `window`]\n"," |          for each target word during training, to match the original word2vec algorithm's\n"," |          approximate weighting of context words by distance. Otherwise, the effective\n"," |          window size is always fixed to `window` words to either side.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      Initialize and train a `FastText` model:\n"," |      \n"," |      .. sourcecode:: pycon\n"," |      \n"," |          >>> from gensim.models import FastText\n"," |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n"," |          >>>\n"," |          >>> model = FastText(sentences, min_count=1)\n"," |          >>> say_vector = model.wv['say']  # get vector for word\n"," |          >>> of_vector = model.wv['of']  # get vector for out-of-vocab word\n"," |      \n"," |      Attributes\n"," |      ----------\n"," |      wv : :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n"," |          This object essentially contains the mapping between words and embeddings. These are similar to\n"," |          the embedding computed in the :class:`~gensim.models.word2vec.Word2Vec`, however here we also\n"," |          include vectors for n-grams. This allows the model to compute embeddings even for **unseen**\n"," |          words (that do not exist in the vocabulary), as the aggregate of the n-grams included in the word.\n"," |          After training the model, this attribute can be used directly to query those embeddings in various\n"," |          ways. Check the module level docstring for some examples.\n"," |  \n"," |  estimate_memory(self, vocab_size=None, report=None)\n"," |      Estimate memory that will be needed to train a model, and print the estimates to log.\n"," |  \n"," |  init_sims(self, replace=False)\n"," |      Precompute L2-normalized vectors. Obsoleted.\n"," |      \n"," |      If you need a single unit-normalized vector for some key, call\n"," |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n"," |      ``fasttext_model.wv.get_vector(key, norm=True)``.\n"," |      \n"," |      To refresh norms after you performed some atypical out-of-band vector tampering,\n"," |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      replace : bool\n"," |          If True, forget the original trained vectors and only keep the normalized ones.\n"," |          You lose information if you do this.\n"," |  \n"," |  load_binary_data(self, encoding='utf8')\n"," |      Load data from a binary file created by Facebook's native FastText.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      encoding : str, optional\n"," |          Specifies the encoding.\n"," |  \n"," |  save(self, *args, **kwargs)\n"," |      Save the Fasttext model. This saved model can be loaded again using\n"," |      :meth:`~gensim.models.fasttext.FastText.load`, which supports incremental training\n"," |      and getting vectors for out-of-vocabulary words.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      fname : str\n"," |          Store the model to this file.\n"," |      \n"," |      See Also\n"," |      --------\n"," |      :meth:`~gensim.models.fasttext.FastText.load`\n"," |          Load :class:`~gensim.models.fasttext.FastText` model.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Class methods defined here:\n"," |  \n"," |  load(*args, **kwargs) from builtins.type\n"," |      Load a previously saved `FastText` model.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      fname : str\n"," |          Path to the saved file.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`~gensim.models.fasttext.FastText`\n"," |          Loaded model.\n"," |      \n"," |      See Also\n"," |      --------\n"," |      :meth:`~gensim.models.fasttext.FastText.save`\n"," |          Save :class:`~gensim.models.fasttext.FastText` model.\n"," |  \n"," |  load_fasttext_format(model_file, encoding='utf8') from builtins.type\n"," |      Deprecated.\n"," |      \n"," |      Use :func:`gensim.models.fasttext.load_facebook_model` or\n"," |      :func:`gensim.models.fasttext.load_facebook_vectors` instead.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from gensim.models.word2vec.Word2Vec:\n"," |  \n"," |  __str__(self)\n"," |      Human readable representation of the model's state.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      str\n"," |          Human readable representation of the model's state, including the vocabulary size, vector size\n"," |          and learning rate.\n"," |  \n"," |  add_null_word(self)\n"," |  \n"," |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n"," |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      corpus_iterable : iterable of list of str\n"," |          Can be simply a list of lists of tokens, but for larger corpora,\n"," |          consider an iterable that streams the sentences directly from disk/network.\n"," |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n"," |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n"," |      corpus_file : str, optional\n"," |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n"," |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n"," |          `corpus_file` arguments need to be passed (not both of them).\n"," |      update : bool\n"," |          If true, the new words in `sentences` will be added to model's vocab.\n"," |      progress_per : int, optional\n"," |          Indicates how many words to process before showing/updating the progress.\n"," |      keep_raw_vocab : bool, optional\n"," |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n"," |      trim_rule : function, optional\n"," |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n"," |          be trimmed away, or handled using the default (discard if word count < min_count).\n"," |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n"," |          or a callable that accepts parameters (word, count, min_count) and returns either\n"," |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n"," |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n"," |          of the model.\n"," |      \n"," |          The input parameters are of the following types:\n"," |              * `word` (str) - the word we are examining\n"," |              * `count` (int) - the word's frequency count in the corpus\n"," |              * `min_count` (int) - the minimum count threshold.\n"," |      \n"," |      **kwargs : object\n"," |          Keyword arguments propagated to `self.prepare_vocab`.\n"," |  \n"," |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n"," |      Build vocabulary from a dictionary of word frequencies.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      word_freq : dict of (str, int)\n"," |          A mapping from a word in the vocabulary to its frequency count.\n"," |      keep_raw_vocab : bool, optional\n"," |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n"," |      corpus_count : int, optional\n"," |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n"," |      trim_rule : function, optional\n"," |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n"," |          be trimmed away, or handled using the default (discard if word count < min_count).\n"," |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n"," |          or a callable that accepts parameters (word, count, min_count) and returns either\n"," |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n"," |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n"," |          of the model.\n"," |      \n"," |          The input parameters are of the following types:\n"," |              * `word` (str) - the word we are examining\n"," |              * `count` (int) - the word's frequency count in the corpus\n"," |              * `min_count` (int) - the minimum count threshold.\n"," |      \n"," |      update : bool, optional\n"," |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n"," |  \n"," |  create_binary_tree(self)\n"," |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n"," |      word counts. Frequent words will have shorter binary codes.\n"," |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n"," |  \n"," |  get_latest_training_loss(self)\n"," |      Get current value of the training loss.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      float\n"," |          Current training loss.\n"," |  \n"," |  init_weights(self)\n"," |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n"," |  \n"," |  make_cum_table(self, domain=2147483647)\n"," |      Create a cumulative-distribution table using stored vocabulary word counts for\n"," |      drawing random words in the negative-sampling training routines.\n"," |      \n"," |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n"," |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n"," |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n"," |  \n"," |  predict_output_word(self, context_words_list, topn=10)\n"," |      Get the probability distribution of the center word given context words.\n"," |      \n"," |      Note this performs a CBOW-style propagation, even in SG models,\n"," |      and doesn't quite weight the surrounding words the same as in\n"," |      training -- so it's just one crude way of using a trained model\n"," |      as a predictor.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      context_words_list : list of (str and/or int)\n"," |          List of context words, which may be words themselves (str)\n"," |          or their index in `self.wv.vectors` (int).\n"," |      topn : int, optional\n"," |          Return `topn` words and their probabilities.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      list of (str, float)\n"," |          `topn` length list of tuples of (word, probability).\n"," |  \n"," |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n"," |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n"," |      and `sample` (controlling the downsampling of more-frequent words).\n"," |      \n"," |      Calling with `dry_run=True` will only simulate the provided settings and\n"," |      report the size of the retained vocabulary, effective corpus length, and\n"," |      estimated memory requirements. Results are both printed via logging and\n"," |      returned as a dict.\n"," |      \n"," |      Delete the raw vocabulary after the scaling is done to free up RAM,\n"," |      unless `keep_raw_vocab` is set.\n"," |  \n"," |  prepare_weights(self, update=False)\n"," |      Build tables and model weights based on final vocabulary settings.\n"," |  \n"," |  reset_from(self, other_model)\n"," |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n"," |      \n"," |      Structures copied are:\n"," |          * Vocabulary\n"," |          * Index to word mapping\n"," |          * Cumulative frequency table (used for negative sampling)\n"," |          * Cached corpus length\n"," |      \n"," |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n"," |      then share all vocabulary-related structures other than vectors, neither should then\n"," |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n"," |      And, any changes to any per-word 'vecattr' will affect both models.\n"," |      \n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n"," |          Another model to copy the internal structures from.\n"," |  \n"," |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n"," |  \n"," |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n"," |      Score the log probability for a sequence of sentences.\n"," |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n"," |      \n"," |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n"," |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n"," |      \n"," |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n"," |      score more than this number of sentences but it is inefficient to set the value too high.\n"," |      \n"," |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n"," |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n"," |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n"," |      how to use such scores in document classification.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      sentences : iterable of list of str\n"," |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n"," |          consider an iterable that streams the sentences directly from disk/network.\n"," |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n"," |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n"," |      total_sentences : int, optional\n"," |          Count of sentences.\n"," |      chunksize : int, optional\n"," |          Chunksize of jobs\n"," |      queue_factor : int, optional\n"," |          Multiplier for size of queue (number of workers * queue_factor).\n"," |      report_delay : float, optional\n"," |          Seconds to wait before reporting progress.\n"," |  \n"," |  seeded_vector(self, seed_string, vector_size)\n"," |  \n"," |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n"," |      Update the model's neural weights from a sequence of sentences.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n"," |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n"," |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n"," |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n"," |      you can simply use `total_examples=self.corpus_count`.\n"," |      \n"," |      Warnings\n"," |      --------\n"," |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n"," |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n"," |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      corpus_iterable : iterable of list of str\n"," |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n"," |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n"," |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n"," |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n"," |          See also the `tutorial on data streaming in Python\n"," |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n"," |      corpus_file : str, optional\n"," |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n"," |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n"," |          `corpus_file` arguments need to be passed (not both of them).\n"," |      total_examples : int\n"," |          Count of sentences.\n"," |      total_words : int\n"," |          Count of raw words in sentences.\n"," |      epochs : int\n"," |          Number of iterations (epochs) over the corpus.\n"," |      start_alpha : float, optional\n"," |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n"," |          for this one call to`train()`.\n"," |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n"," |          (not recommended).\n"," |      end_alpha : float, optional\n"," |          Final learning rate. Drops linearly from `start_alpha`.\n"," |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n"," |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n"," |          (not recommended).\n"," |      word_count : int, optional\n"," |          Count of words already trained. Set this to 0 for the usual\n"," |          case of training on all words in sentences.\n"," |      queue_factor : int, optional\n"," |          Multiplier for size of queue (number of workers * queue_factor).\n"," |      report_delay : float, optional\n"," |          Seconds to wait before reporting progress.\n"," |      compute_loss: bool, optional\n"," |          If True, computes and stores loss value which can be retrieved using\n"," |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n"," |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n"," |          Sequence of callbacks to be executed at specific stages during training.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      .. sourcecode:: pycon\n"," |      \n"," |          >>> from gensim.models import Word2Vec\n"," |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n"," |          >>>\n"," |          >>> model = Word2Vec(min_count=1)\n"," |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n"," |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n"," |          (1, 30)\n"," |  \n"," |  update_weights(self)\n"," |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from gensim.utils.SaveLoad:\n"," |  \n"," |  add_lifecycle_event(self, event_name, log_level=20, **event)\n"," |      Append an event into the `lifecycle_events` attribute of this object, and also\n"," |      optionally log the event at `log_level`.\n"," |      \n"," |      Events are important moments during the object's life, such as \"model created\",\n"," |      \"model saved\", \"model loaded\", etc.\n"," |      \n"," |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n"," |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n"," |      but is useful during debugging and support.\n"," |      \n"," |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n"," |      will not record events into `self.lifecycle_events` then.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      event_name : str\n"," |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n"," |      event : dict\n"," |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n"," |          Can be empty.\n"," |      \n"," |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n"," |      \n"," |          - `datetime`: the current date & time\n"," |          - `gensim`: the current Gensim version\n"," |          - `python`: the current Python version\n"," |          - `platform`: the current platform\n"," |          - `event`: the name of this event\n"," |      log_level : int\n"," |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from gensim.utils.SaveLoad:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n","\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"dw9JpI6AgV6e","executionInfo":{"status":"ok","timestamp":1718916999123,"user_tz":-120,"elapsed":548,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"outputs":[],"source":["# Skip Gram\n","ft_sg = FastText(**sg_params)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZVumppbugV6e"},"source":["## Construímos el vocabulario"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"nEahkKoUgV6e","executionInfo":{"status":"ok","timestamp":1718917009949,"user_tz":-120,"elapsed":4894,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"outputs":[],"source":["# Skip Gram\n","ft_sg.build_vocab(sent)\n","\n"]},{"cell_type":"code","source":["print('Vocabulario compuesto por {} palabras'.format(len(ft_sg.wv.key_to_index)))\n"],"metadata":{"id":"CfwtYvJoQgGi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718917009950,"user_tz":-120,"elapsed":2,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"2a9c7958-b5f6-4e27-edab-2599a304491a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulario compuesto por 6130 palabras\n"]}]},{"cell_type":"markdown","metadata":{"id":"NlXG1UfHgV6f"},"source":["## Entrenamos los pesos de los embeddings"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Sz1U_c4pgV6f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718917188572,"user_tz":-120,"elapsed":172076,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"8ef7f717-5bf8-41c8-e7fa-402e6cfe5c81"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5024696, 6116860)"]},"metadata":{},"execution_count":12}],"source":["# Skip Gram\n","\n","\n","ft_sg.train(sent, total_examples=len(sent), epochs=20)\n"]},{"cell_type":"markdown","metadata":{"id":"-F62S0H2gV6f"},"source":["## Guardamos los modelos"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"eTt2zNitgV6f","executionInfo":{"status":"ok","timestamp":1718917228381,"user_tz":-120,"elapsed":27479,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}}},"outputs":[],"source":["ft_sg.save('./w2v_model_fast.pkl')\n"]},{"cell_type":"markdown","metadata":{"id":"-79KWIfjgV6f"},"source":["## Algunos resultados"]},{"cell_type":"code","source":["ft_sg.wv.most_similar(positive=[\"homer\"])"],"metadata":{"id":"878nHw2fQjPM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718917232177,"user_tz":-120,"elapsed":686,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"7cf31761-3607-4dd1-d283-1a822e7d85b0"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('knockahomer', 0.6621387600898743),\n"," ('homey', 0.6509182453155518),\n"," ('astronomer', 0.5661692023277283),\n"," ('customer', 0.5487726330757141),\n"," ('homemade', 0.5278177261352539),\n"," ('mer', 0.5139923691749573),\n"," ('carrier', 0.5022217631340027),\n"," ('home', 0.5019287467002869),\n"," ('somewhat', 0.5005836486816406),\n"," ('margarita', 0.49495843052864075)]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["ft_sg.wv.most_similar(positive=[\"marge\"])"],"metadata":{"id":"X6IPOQj_QjRY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718917329501,"user_tz":-120,"elapsed":451,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"6a1610d1-5fa6-4f74-9dcf-3cf41dcc2570"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('sarge', 0.7090577483177185),\n"," ('margarita', 0.7005890011787415),\n"," ('margie', 0.6558294892311096),\n"," ('marjorie', 0.5643143653869629),\n"," ('large', 0.5106120705604553),\n"," ('married', 0.5083991289138794),\n"," ('urge', 0.5059846043586731),\n"," ('marriage', 0.4967114329338074),\n"," ('argue', 0.4876605272293091),\n"," ('march', 0.4851870536804199)]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["ft_sg.wv.most_similar(positive=[\"bart\"])"],"metadata":{"id":"579VgWJHQpgG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718917345476,"user_tz":-120,"elapsed":343,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"002711e4-3fae-44bf-c4ec-ccd4d8e12084"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('barty', 0.6337318420410156),\n"," ('bartron', 0.5803127288818359),\n"," ('barf', 0.5655282139778137),\n"," ('bartholomew', 0.5616724491119385),\n"," ('dart', 0.5523513555526733),\n"," ('baryshnikov', 0.5475201606750488),\n"," ('fart', 0.5405943393707275),\n"," ('barbara', 0.5387760400772095),\n"," ('gypsy', 0.522591769695282),\n"," ('art', 0.5124298334121704)]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["ft_sg.wv.similarity('maggie', 'baby')"],"metadata":{"id":"ATqRO5LoQsLz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718917350594,"user_tz":-120,"elapsed":319,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"edcf9350-e43c-44d4-dc90-25b33f77e385"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.40071344"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["ft_sg.wv.similarity('bart', 'nelson')"],"metadata":{"id":"qXCHTWzKQvUd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718917371749,"user_tz":-120,"elapsed":329,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"7217987b-76c1-43f5-c075-0cd3b70e7f30"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.34023723"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["ft_sg.wv.doesnt_match(['jimbo', 'milhouse', 'kearney'])"],"metadata":{"id":"os5efiscQxk5","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1718917374025,"user_tz":-120,"elapsed":3,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"4534d208-6ce0-4973-bc8e-97b081256201"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'milhouse'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["ft_sg.wv.doesnt_match(['homer', 'patty', 'selma'])"],"metadata":{"id":"vu_WWM9mQ0TO","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1718917375789,"user_tz":-120,"elapsed":3,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"5ebad8d0-d1cb-4eb9-8108-a660a77a23ba"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'homer'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"uivGUy4AgV6g"},"source":["## Out-of-Vocabulary (OOV) Words"]},{"cell_type":"markdown","metadata":{"id":"enFbsjjNgV6g"},"source":["la cantidad de n-grams creados durante el entrenamiento del FastText hace improbable (que no imposible) que alguna palabra no pueda ser construída como una bolsa de n-grams"]},{"cell_type":"code","source":["'asereje' in ft_sg.wv.key_to_index"],"metadata":{"id":"X4rs_XO7Q2wX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718917386810,"user_tz":-120,"elapsed":2,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"0a785ea1-20e5-4998-9eb2-a02100776f6b"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["ft_sg.wv.most_similar('asereje')"],"metadata":{"id":"YzL_dhM-Q5j8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718917392125,"user_tz":-120,"elapsed":316,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"f6769af9-eb53-4c09-aede-14b2f5f24c45"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('eraser', 0.6586302518844604),\n"," ('taser', 0.657060444355011),\n"," ('lu', 0.6406767964363098),\n"," ('eliza', 0.6382820010185242),\n"," ('shredded', 0.6241763234138489),\n"," ('laser', 0.6193310022354126),\n"," ('cease', 0.6184722185134888),\n"," ('whereabouts', 0.6139325499534607),\n"," ('buddhist', 0.6138971447944641),\n"," ('huzzah', 0.6137781143188477)]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["ft_sg.wv['asereje'].shape"],"metadata":{"id":"wA7HOBBCQ8Ux","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718917402060,"user_tz":-120,"elapsed":376,"user":{"displayName":"Cristina Gomez","userId":"02340994425751932170"}},"outputId":"f9f13448-e1c5-471b-9ad0-56ea345bed36"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(300,)"]},"metadata":{},"execution_count":23}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[{"file_id":"1QMOQoBWaOAU-aHNLOD1emZyB2v_yLU4J","timestamp":1677087466345}]}},"nbformat":4,"nbformat_minor":0}